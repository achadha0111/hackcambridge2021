{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data to be scraped:\n",
    "1. News Headline \n",
    "2. Rating \n",
    "3. News Outlet \n",
    "4. URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen as ureq\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coronavirus Headlines\n",
    "Scraping COVID-19 articles from 'allsides.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.allsides.com/topics/coronavirus?search=coronavirus#gsc.tab=0&gsc.q=coronavirus&gsc.page=1'\n",
    "uclient = ureq(url)\n",
    "page_html = uclient.read()\n",
    "uclient.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# html parsing\n",
    "soup = BeautifulSoup(page_html, \"html.parser\")\n",
    "\n",
    "# grabs each grid\n",
    "containers = soup.findAll(\"div\", {\"class\":\"topic_combine\"})\n",
    "\n",
    "article_links = []\n",
    "article_headlines = []\n",
    "\n",
    "\n",
    "for link in containers:\n",
    "    headlines = link.findAll(\"div\", {\"class\":\"news-title\"})\n",
    "    for urls in headlines:\n",
    "        news_title = urls.findAll(\"a\", href=True)\n",
    "        \n",
    "        # Appending the article links to this list\n",
    "        article_links.append(news_title[0]['href'])\n",
    "        \n",
    "        # Appending the article headlines to this list\n",
    "        article_headlines.append(news_title[0].text)\n",
    "        \n",
    "# Scraping the news outlet names\n",
    "\n",
    "agencies = soup.findAll(\"div\",{\"class\": \"source-area\"})\n",
    "\n",
    "article_outlets = []\n",
    "article_ratings =[]\n",
    "\n",
    "for agency in agencies:\n",
    "    outlets = agency.findAll(\"div\", {\"class\":\"news-source\"})\n",
    "    for outlet in outlets:\n",
    "        article_outlet = outlet.findAll(\"a\", href=True)\n",
    "            \n",
    "        # Appending the article outlet\n",
    "        article_outlets.append(article_outlet[0].text)\n",
    "\n",
    "# Scraping the political rating\n",
    "\n",
    "for agency in soup.findAll(\"div\",{\"class\": \"field-content\"}):\n",
    "    for img in agency:\n",
    "        article_ratings.append(img.get('title'))\n",
    "\n",
    "# Making a dataframe that combines all scraped lists\n",
    "\n",
    "Covid_Headlines = DataFrame(\n",
    "    {'Headline': Series(article_headlines),\n",
    "     'Outlet': Series(article_outlets),\n",
    "     'Rating': Series(article_ratings),\n",
    "     'Link': Series(article_links)\n",
    "    })\n",
    "\n",
    "Covid_Headlines.to_csv('Covid_Headlines.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Climate Change Headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = 'https://www.allsides.com/topics/climate-change?search=climate%20change#gsc.tab=0&gsc.q=climate%20change&gsc.page=1'\n",
    "uclient1 = ureq(url1)\n",
    "page_html1 = uclient1.read()\n",
    "uclient1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# html parsing\n",
    "soup1 = BeautifulSoup(page_html1, \"html.parser\")\n",
    "\n",
    "# grabs each grid\n",
    "containers1 = soup1.findAll(\"div\", {\"class\":\"topic_combine\"})\n",
    "\n",
    "climate_links = []\n",
    "climate_headlines = []\n",
    "\n",
    "\n",
    "for link1 in containers1:\n",
    "    headlines1 = link1.findAll(\"div\", {\"class\":\"news-title\"})\n",
    "    for urls1 in headlines1:\n",
    "        news_title1 = urls1.findAll(\"a\", href=True)\n",
    "        \n",
    "        # Appending the article links to this list\n",
    "        climate_links.append(news_title1[0]['href'])\n",
    "        \n",
    "        # Appending the article headlines to this list\n",
    "        climate_headlines.append(news_title1[0].text)\n",
    "        \n",
    "# Scraping the news outlet names\n",
    "\n",
    "agencies1 = soup1.findAll(\"div\",{\"class\": \"source-area\"})\n",
    "\n",
    "climate_outlets = []\n",
    "climate_ratings =[]\n",
    "\n",
    "for agency1 in agencies1:\n",
    "    outlets1 = agency1.findAll(\"div\", {\"class\":\"news-source\"})\n",
    "    for outlet1 in outlets1:\n",
    "        article_outlet1 = outlet1.findAll(\"a\", href=True)\n",
    "            \n",
    "        # Appending the article outlet\n",
    "        climate_outlets.append(article_outlet1[0].text)\n",
    "\n",
    "# Scraping the political rating\n",
    "\n",
    "for agency1 in soup1.findAll(\"div\",{\"class\": \"field-content\"}):\n",
    "    for img1 in agency1:\n",
    "        climate_ratings.append(img1.get('title'))\n",
    "\n",
    "# Making a dataframe that combines all scraped lists\n",
    "\n",
    "Climate_Headlines = DataFrame(\n",
    "    {'Headline': Series(climate_headlines),\n",
    "     'Outlet': Series(climate_outlets),\n",
    "     'Rating': Series(climate_ratings),\n",
    "     'Link': Series(climate_links)\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "Climate_Headlines.to_csv('Climate_Headlines.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free Speech "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url2 = 'https://www.allsides.com/topics/free-speech?search=free%20speech#gsc.tab=0&gsc.q=free%20speech&gsc.page=1'\n",
    "uclient2 = ureq(url2)\n",
    "page_html2 = uclient2.read()\n",
    "uclient2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# html parsing\n",
    "soup2 = BeautifulSoup(page_html2, \"html.parser\")\n",
    "\n",
    "# grabs each grid\n",
    "containers2 = soup2.findAll(\"div\", {\"class\":\"topic_combine\"})\n",
    "\n",
    "speech_links = []\n",
    "speech_headlines = []\n",
    "\n",
    "\n",
    "for link2 in containers2:\n",
    "    headlines2 = link2.findAll(\"div\", {\"class\":\"news-title\"})\n",
    "    for urls2 in headlines2:\n",
    "        news_title2 = urls2.findAll(\"a\", href=True)\n",
    "        \n",
    "        # Appending the article links to this list\n",
    "        speech_links.append(news_title2[0]['href'])\n",
    "        \n",
    "        # Appending the article headlines to this list\n",
    "        speech_headlines.append(news_title2[0].text)\n",
    "        \n",
    "# Scraping the news outlet names\n",
    "\n",
    "agencies2 = soup2.findAll(\"div\",{\"class\": \"source-area\"})\n",
    "\n",
    "speech_outlets = []\n",
    "speech_ratings =[]\n",
    "\n",
    "for agency2 in agencies2:\n",
    "    outlets2 = agency2.findAll(\"div\", {\"class\":\"news-source\"})\n",
    "    for outlet2 in outlets2:\n",
    "        article_outlet2 = outlet2.findAll(\"a\", href=True)\n",
    "            \n",
    "        # Appending the article outlet\n",
    "        speech_outlets.append(article_outlet2[0].text)\n",
    "\n",
    "# Scraping the political rating\n",
    "\n",
    "for agency2 in soup2.findAll(\"div\",{\"class\": \"field-content\"}):\n",
    "    for img2 in agency2:\n",
    "        speech_ratings.append(img2.get('title'))\n",
    "\n",
    "# Making a dataframe that combines all scraped lists\n",
    "\n",
    "Speech_Headlines = DataFrame(\n",
    "    {'Headline': Series(speech_headlines),\n",
    "     'Outlet': Series(speech_outlets),\n",
    "     'Rating': Series(speech_ratings),\n",
    "     'Link': Series(speech_links)\n",
    "    })\n",
    "\n",
    "Speech_Headlines.to_csv('Speech_Headlines.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
